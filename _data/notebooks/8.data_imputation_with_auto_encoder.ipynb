{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mushroom autoencoder image\n",
    "\n",
    "Mushrooms, anyone? What if you have lots of data on mushrooms, yet some of it is missing? One important question you might want to answer is whether or not a particular specimen is edible or poisonous. Of course, your understanding of what a poisonous mushroom is might be quite different (hi to all from Netherlands), but I digress.\n",
    "\n",
    "The dataset of interest will be (you guessed it) all about mushrooms. It describes physical characteristics of *8124* mushroom instances. The number of variables is 23, all of which are categorical. More information about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Mushroom).\n",
    "\n",
    "# Autoencoders\n",
    "\n",
    "Strangely enough, an autoencoder is a model that given input data tries to predict it. It is used for unsupervised learning (That might not be entirely correct). Puzzling? First time I heard the concept I thought it must be a misunderstanding on my part. It wasn't.\n",
    "\n",
    "More specifically, let's take a look at Autoencoder Neural Networks. This autoencoder tries to learn to approximate the following identity function:\n",
    "\n",
    "$$\\textstyle f_{W,b}(x) \\approx x$$\n",
    "\n",
    "While trying to do just that might sound trivial at first, it is important to note that we want to learn a compressed representation of the data, thus find structure. This can be done by limiting the number of hidden units in the model. Those kind of autoencoders are called *undercomplete*.\n",
    "\n",
    "## Choosing loss function\n",
    "\n",
    "In order to learn something meaningful, autoencoders should try to minimize some function, called *reconstruction error*. The traditional *squared error* is often used:\n",
    "\n",
    "$$\\textstyle L(x,x') = ||\\, x - x'||^2$$\n",
    "\n",
    "# Creating an Autoencoder\n",
    "\n",
    "We will use Keras to create a simple Deep Autoencoder. Before getting to the fun part, though, we have some housekeeping to do. How should we encode the categorical data along with the missing values? Will that affect our reconstruction error?\n",
    "\n",
    "## Encoding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.objectives import mse\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.regularizers import l1l2\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reverse_dummy(df_dummies):\n",
    "    pos = defaultdict(list)\n",
    "    vals = defaultdict(list)\n",
    "\n",
    "    for i, c in enumerate(df_dummies.columns):\n",
    "        if \"_\" in c:\n",
    "            k, v = c.split(\"_\", 1)\n",
    "            pos[k].append(i)\n",
    "            vals[k].append(v)\n",
    "        else:\n",
    "            pos[\"_\"].append(i)\n",
    "\n",
    "    df = pd.DataFrame({k: pd.Categorical.from_codes(\n",
    "                              np.argmax(df_dummies.iloc[:, pos[k]].values, axis=1),\n",
    "                              vals[k])\n",
    "                      for k in vals})\n",
    "\n",
    "    df[df_dummies.columns[pos[\"_\"]]] = df_dummies.iloc[:, pos[\"_\"]]\n",
    "    return df\n",
    "\n",
    "def make_reconstruction_loss(n_features):\n",
    "\n",
    "    def reconstruction_loss(input_and_mask, y_pred):\n",
    "        X_values = input_and_mask[:, :n_features]\n",
    "        X_values.name = \"$X_values\"\n",
    "\n",
    "        missing_mask = input_and_mask[:, n_features:]\n",
    "        missing_mask.name = \"$missing_mask\"\n",
    "        observed_mask = 1 - missing_mask\n",
    "        observed_mask.name = \"$observed_mask\"\n",
    "\n",
    "        X_values_observed = X_values * observed_mask\n",
    "        X_values_observed.name = \"$X_values_observed\"\n",
    "\n",
    "        pred_observed = y_pred * observed_mask\n",
    "        pred_observed.name = \"$y_pred_observed\"\n",
    "\n",
    "        return mse(y_true=X_values_observed, y_pred=pred_observed)\n",
    "    return reconstruction_loss\n",
    "\n",
    "def masked_mae(X_true, X_pred, mask):\n",
    "    masked_diff = X_true[mask] - X_pred[mask]\n",
    "    return np.mean(np.abs(masked_diff))\n",
    "\n",
    "def mle(row):\n",
    "    res = np.zeros(row.shape[0])\n",
    "    res[np.argmax(row)] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 recurrent_weight = 0.5):\n",
    "        self.data = data\n",
    "        self.recurrent_weight = recurrent_weight\n",
    "\n",
    "    def _create_model(self):\n",
    "        n_dims = self.data.shape[1]\n",
    "        hidden_layer_sizes = [\n",
    "            min(2000, 8 * n_dims),\n",
    "            min(500, 2 * n_dims),\n",
    "            int(np.ceil(0.5 * n_dims)),\n",
    "        ]\n",
    "\n",
    "        first_layer_size = hidden_layer_sizes[0]\n",
    "\n",
    "        hidden_activation = 'relu'\n",
    "        output_activation = 'sigmoid'\n",
    "        init=\"glorot_normal\"\n",
    "        l1_penalty = 0\n",
    "        l2_penalty = 0\n",
    "        dropout_probability=0.5\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(\n",
    "            first_layer_size,\n",
    "            input_dim= 2 * n_dims,\n",
    "            activation=hidden_activation,\n",
    "            W_regularizer=l1l2(l1_penalty, l2_penalty),\n",
    "            init=init))\n",
    "        model.add(Dropout(dropout_probability))\n",
    "\n",
    "        for layer_size in hidden_layer_sizes[1:]:\n",
    "            model.add(Dense(\n",
    "                layer_size,\n",
    "                activation=hidden_activation,\n",
    "                W_regularizer=l1l2(l1_penalty, l2_penalty),\n",
    "                init=init))\n",
    "            model.add(Dropout(dropout_probability))\n",
    "\n",
    "        model.add(Dense(\n",
    "            n_dims,\n",
    "            activation=output_activation,\n",
    "            W_regularizer=l1l2(l1_penalty, l2_penalty),\n",
    "            init=init))\n",
    "\n",
    "        loss_function = make_reconstruction_loss(n_dims)\n",
    "\n",
    "        optimizer = \"adam\"\n",
    "        model.compile(optimizer=optimizer, loss=loss_function)\n",
    "        return model\n",
    "\n",
    "    def fill(self, missing_mask):\n",
    "        self.data[missing_mask] = -1\n",
    "\n",
    "    def _create_missing_mask(self):\n",
    "        if self.data.dtype != \"f\" and self.data.dtype != \"d\":\n",
    "            self.data = self.data.astype(float)\n",
    "\n",
    "        return np.isnan(self.data)\n",
    "\n",
    "    def _train_epoch(self, model, missing_mask, batch_size):\n",
    "        input_with_mask = np.hstack([self.data, missing_mask])\n",
    "        n_samples = len(input_with_mask)\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = input_with_mask[indices]\n",
    "\n",
    "        for batch_idx in range(n_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
    "            model.train_on_batch(batch_data, batch_data)\n",
    "        return model.predict(input_with_mask)\n",
    "\n",
    "    def train(self, batch_size=256, train_epochs=100):\n",
    "        missing_mask = self._create_missing_mask()\n",
    "        self.fill(missing_mask)\n",
    "        model = self._create_model()\n",
    "\n",
    "        observed_mask = ~missing_mask\n",
    "\n",
    "        for _ in range(train_epochs):\n",
    "            X_pred = self._train_epoch(model, missing_mask, batch_size)\n",
    "            observed_mae = masked_mae(X_true=self.data,\n",
    "                                    X_pred=X_pred,\n",
    "                                    mask=observed_mask)\n",
    "            print(\"observed mae:\", observed_mae)\n",
    "\n",
    "            old_weight = (1.0 - self.recurrent_weight)\n",
    "            self.data[missing_mask] *= old_weight\n",
    "            pred_missing = X_pred[missing_mask]\n",
    "            self.data[missing_mask] += self.recurrent_weight * pred_missing\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed mae: 0.249136511569\n",
      "observed mae: 0.1506028165\n",
      "observed mae: 0.12032281598\n",
      "observed mae: 0.107268109797\n",
      "observed mae: 0.097324605074\n",
      "observed mae: 0.0945542617275\n",
      "observed mae: 0.0858770286434\n",
      "observed mae: 0.0807686920289\n",
      "observed mae: 0.0759420838568\n",
      "observed mae: 0.0723345458635\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "df = df.drop(['sroot'], axis=1)\n",
    "df_incomplete = df.copy()\n",
    "import random\n",
    "ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "for row, col in random.sample(ix, int(round(.1*len(ix)))):\n",
    "    df_incomplete.iat[row, col] = np.nan\n",
    "dummy_df = pd.get_dummies(df)\n",
    "dummy_incomplete_df = pd.get_dummies(df_incomplete)\n",
    "\n",
    "for col in df.columns:\n",
    "    dummy_incomplete_df.loc[df_incomplete[col].isnull(), dummy_incomplete_df.columns.str.startswith(str(col) + \"_\")] = np.nan\n",
    "\n",
    "imputer = Autoencoder(dummy_incomplete_df.copy().values)\n",
    "imputer.train(train_epochs=10)\n",
    "\n",
    "dummy_completed_df = imputer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769876349801\n"
     ]
    }
   ],
   "source": [
    "col_classes = [len(df[c].unique()) for c in df.columns]\n",
    "\n",
    "mle_complete_df = None\n",
    "\n",
    "for i, cnt in enumerate(col_classes):\n",
    "    start_idx = int(sum(col_classes[0:i]))\n",
    "    col_true = dummy_df.values[:, start_idx:start_idx+cnt]\n",
    "    col_completed = dummy_completed_df[:, start_idx:start_idx+cnt]\n",
    "    mle_completed = np.apply_along_axis(mle, axis=1, arr=col_completed)\n",
    "    if mle_complete_df is None:\n",
    "        mle_complete_df = mle_completed\n",
    "    else:\n",
    "        mle_complete_df = np.hstack([mle_complete_df, mle_completed])\n",
    "\n",
    "rev_df = reverse_dummy(pd.DataFrame(data=mle_complete_df, columns=dummy_df.columns))\n",
    "rev_df = rev_df[list(df.columns)]\n",
    "incorrect = (rev_df != df)\n",
    "incorrect_cnts = incorrect.apply(pd.value_counts)\n",
    "incorrect_sum = incorrect_cnts.sum(axis=1)\n",
    "\n",
    "missing = df_incomplete.apply(pd.isnull)\n",
    "missing_cnts = missing.apply(pd.value_counts)\n",
    "missing_sum = missing_cnts.sum(axis=1)\n",
    "\n",
    "accuracy = 1.0 - (incorrect_sum[1] / missing_sum[1])\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
