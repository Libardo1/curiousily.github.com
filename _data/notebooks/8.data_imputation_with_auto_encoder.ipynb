{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mushrooms, anyone? What if you have lots of data on mushrooms, yet some of it is missing? One important question you might want to answer is whether or not a particular specimen is edible or poisonous. Of course, your understanding of what a poisonous mushroom is might be quite different (hi to all from Netherlands), but I digress.\n",
    "\n",
    "The dataset of interest will be (you guessed it) all about mushrooms. It describes physical characteristics of *8124* mushroom instances. The number of variables is 23, all of which are categorical. More information about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Mushroom)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Strangely enough, an autoencoder is a model that given input data tries to predict it. It is used for unsupervised learning (That might not be entirely correct). Puzzling? First time I heard the concept I thought it must be a misunderstanding on my part. It wasn't.\n",
    "\n",
    "More specifically, let's take a look at Autoencoder Neural Networks. This autoencoder tries to learn to approximate the following identity function:\n",
    "\n",
    "$$\\textstyle f_{W,b}(x) \\approx x$$\n",
    "\n",
    "While trying to do just that might sound trivial at first, it is important to note that we want to learn a compressed representation of the data, thus find structure. This can be done by limiting the number of hidden units in the model. Those kind of autoencoders are called *undercomplete*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1 Choosing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to learn something meaningful, autoencoders should try to minimize some function, called *reconstruction error*. The traditional *squared error* is often used:\n",
    "\n",
    "$$\\textstyle L(x,x') = ||\\, x - x'||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Creating an Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use Keras to create a simple Deep Autoencoder. Before getting to the fun part, though, we have some housekeeping to do. How should we encode the categorical data along with the missing values? Will that affect our reconstruction error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.1 Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our dataset contains categorical variables exclusively. We will use a standard approach for such cases - one-hot encoding. Furthermore, we have to handle cells with missing values. We will create a missing mask vector and append it to our one-hot encoded values. Missing values will be filled with some constant.\n",
    "\n",
    "Let's take a look at this sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[\n",
    "    ['blue stem', 'large cap'],\n",
    "    [None, 'large cap'],\n",
    "    ['green stem', None]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our encoded data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0, 0], # no missing values\n",
    "    [0, 0, 1, 1, 0, 1, 1, 1, 0, 0], # missing value in 1st variable\n",
    "    [0, 1, 0, 0, 1, 0, 0, 0, 1, 1]  # missing value in 2nd variable\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2 Reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our reconstruction error will be the mean squared error which hopefully will work for categorical data. Since we are using Keras, our function must adhere to some rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_reconstruction_loss(n_features):\n",
    "\n",
    "    def reconstruction_loss(input_and_mask, y_pred):\n",
    "        X_values = input_and_mask[:, :n_features]\n",
    "        X_values.name = \"$X_values\"\n",
    "\n",
    "        missing_mask = input_and_mask[:, n_features:]\n",
    "        missing_mask.name = \"$missing_mask\"\n",
    "        observed_mask = 1 - missing_mask\n",
    "        observed_mask.name = \"$observed_mask\"\n",
    "\n",
    "        X_values_observed = X_values * observed_mask\n",
    "        X_values_observed.name = \"$X_values_observed\"\n",
    "\n",
    "        pred_observed = y_pred * observed_mask\n",
    "        pred_observed.name = \"$y_pred_observed\"\n",
    "\n",
    "        return mse(y_true=X_values_observed, y_pred=pred_observed)\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will use slightly modified mean squared error for assessing our progress during training. The function takes into account the mask of the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_mae(X_true, X_pred, mask):\n",
    "    masked_diff = X_true[mask] - X_pred[mask]\n",
    "    return np.mean(np.abs(masked_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.3 Getting everything just right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Of course, a couple of imports are needed to make everything work. Make sure you have the proper libraries installed (all available via pip install)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.objectives import mse\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.regularizers import l1l2\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.4 Creating the model (the fun part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following implementation is heavily based on the one provided by [fancyimpute](https://github.com/hammerlab/fancyimpute/). Our NN has 3 hidden layers (with ReLU activation functions) with dropout probability set to 0.5. You can also choose two regularizers coefficients - the sum of the weights (L1) and sum the squares of the weights (L2). These are 0. The activation function for the output layer is sigmoid. It appears to work better than linear for our use case.\n",
    "\n",
    "Here is the code for it, don't be afraid to fiddle with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 recurrent_weight=0.5,\n",
    "                 optimizer=\"adam\",\n",
    "                 dropout_probability=0.5,\n",
    "                 hidden_activation=\"relu\",\n",
    "                 output_activation=\"sigmoid\",\n",
    "                 init=\"glorot_normal\",\n",
    "                 l1_penalty=0,\n",
    "                 l2_penalty=0):\n",
    "        self.data = data.copy()\n",
    "        self.recurrent_weight = recurrent_weight\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.init = init\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.l2_penalty = l2_penalty\n",
    "        \n",
    "    def _get_hidden_layer_sizes(self):\n",
    "        n_dims = self.data.shape[1]\n",
    "        return [\n",
    "            min(2000, 8 * n_dims),\n",
    "            min(500, 2 * n_dims),\n",
    "            int(np.ceil(0.5 * n_dims)),\n",
    "        ]\n",
    "\n",
    "    def _create_model(self):\n",
    "\n",
    "        hidden_layer_sizes = self._get_hidden_layer_sizes()\n",
    "        first_layer_size = hidden_layer_sizes[0]\n",
    "        n_dims = self.data.shape[1]\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(\n",
    "            first_layer_size,\n",
    "            input_dim= 2 * n_dims,\n",
    "            activation=self.hidden_activation,\n",
    "            W_regularizer=l1l2(self.l1_penalty, self.l2_penalty),\n",
    "            init=self.init))\n",
    "        model.add(Dropout(self.dropout_probability))\n",
    "\n",
    "        for layer_size in hidden_layer_sizes[1:]:\n",
    "            model.add(Dense(\n",
    "                layer_size,\n",
    "                activation=self.hidden_activation,\n",
    "                W_regularizer=l1l2(self.l1_penalty, self.l2_penalty),\n",
    "                init=self.init))\n",
    "            model.add(Dropout(self.dropout_probability))\n",
    "\n",
    "        model.add(Dense(\n",
    "            n_dims,\n",
    "            activation=self.output_activation,\n",
    "            W_regularizer=l1l2(self.l1_penalty, self.l2_penalty),\n",
    "            init=self.init))\n",
    "\n",
    "        loss_function = make_reconstruction_loss(n_dims)\n",
    "\n",
    "        model.compile(optimizer=self.optimizer, loss=loss_function)\n",
    "        return model\n",
    "\n",
    "    def fill(self, missing_mask):\n",
    "        self.data[missing_mask] = -1\n",
    "\n",
    "    def _create_missing_mask(self):\n",
    "        if self.data.dtype != \"f\" and self.data.dtype != \"d\":\n",
    "            self.data = self.data.astype(float)\n",
    "\n",
    "        return np.isnan(self.data)\n",
    "\n",
    "    def _train_epoch(self, model, missing_mask, batch_size):\n",
    "        input_with_mask = np.hstack([self.data, missing_mask])\n",
    "        n_samples = len(input_with_mask)\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = input_with_mask[indices]\n",
    "\n",
    "        for batch_idx in range(n_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
    "            model.train_on_batch(batch_data, batch_data)\n",
    "        return model.predict(input_with_mask)\n",
    "\n",
    "    def train(self, batch_size=256, train_epochs=100):\n",
    "        missing_mask = self._create_missing_mask()\n",
    "        self.fill(missing_mask)\n",
    "        self.model = self._create_model()\n",
    "\n",
    "        observed_mask = ~missing_mask\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            X_pred = self._train_epoch(self.model, missing_mask, batch_size)\n",
    "            observed_mae = masked_mae(X_true=self.data,\n",
    "                                    X_pred=X_pred,\n",
    "                                    mask=observed_mask)\n",
    "            if epoch % 50 == 0:\n",
    "                print(\"observed mae:\", observed_mae)\n",
    "\n",
    "            old_weight = (1.0 - self.recurrent_weight)\n",
    "            self.data[missing_mask] *= old_weight\n",
    "            pred_missing = X_pred[missing_mask]\n",
    "            self.data[missing_mask] += self.recurrent_weight * pred_missing\n",
    "        return self.data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, that was a large chunk of code. The important part is updating our data where values are missing. We use some predefined weight along with the predictions of our NN to update only the missing value cells. Here is a diagram of our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our Autoencoder can impute missing data, shall we?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the encoding technique for our categorical data as discussed above. Let's load (and drop a column with missing values from) our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "df = df.drop(['sroot'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Making data dissapear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MCAR as a driving process behind making missing data. Each data point in our dataset has 0.1 probability of being set to *NaN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_missing = 0.1\n",
    "df_incomplete = df.copy()\n",
    "ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "for row, col in random.sample(ix, int(round(prob_missing * len(ix)))):\n",
    "    df_incomplete.iat[row, col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode our dataset using panda's <code>get_dummies</code> and apply our missing data points accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_encoded = pd.get_dummies(df_incomplete)\n",
    "\n",
    "for col in df.columns:\n",
    "    missing_cols = missing_encoded.columns.str.startswith(str(col) + \"_\")\n",
    "    missing_encoded.loc[df_incomplete[col].isnull(), missing_cols] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Working out our Encoder (or just training it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, it is time to use our skillfully crafted model. We will train it for 300 epochs with a batch size of 256. All else is left to it's default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed mae: 0.230737793827\n",
      "observed mae: 0.0371880909997\n",
      "observed mae: 0.0268712357369\n",
      "observed mae: 0.0232342579129\n",
      "observed mae: 0.0210763975367\n",
      "observed mae: 0.0191748421237\n"
     ]
    }
   ],
   "source": [
    "imputer = Autoencoder(missing_encoded.values)\n",
    "complete_encoded = imputer.train(train_epochs=300, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! Just kidding. The error seems to decrease so it might be wise to train for a few (or much more) epochs. \n",
    "\n",
    "That run surprisingly fast on my 2.6 Ghz i7 (2013) with 16 Gb DDR3 ram. Should definitely try it out on a CUDA-enabled machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 How well we did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to do a little more housekeeping before answering that one. Here is an example of a row from our imputed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.00000000e+00,   4.18084731e-17,\n",
       "         5.83600606e-20])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_encoded[10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not what you expected? That's all right. Let's use maximum likelihood estimation (MLE) to pick winning prediction for each missing data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mle(row):\n",
    "    res = np.zeros(row.shape[0])\n",
    "    res[np.argmax(row)] = 1\n",
    "    return res\n",
    "\n",
    "col_classes = [len(df[c].unique()) for c in df.columns]\n",
    "\n",
    "dummy_df = pd.get_dummies(df)\n",
    "\n",
    "mle_complete = None\n",
    "\n",
    "for i, cnt in enumerate(col_classes):\n",
    "    start_idx = int(sum(col_classes[0:i]))\n",
    "    col_true = dummy_df.values[:, start_idx:start_idx+cnt]\n",
    "    col_completed = complete_encoded[:, start_idx:start_idx+cnt]\n",
    "    mle_completed = np.apply_along_axis(mle, axis=1, arr=col_completed)\n",
    "    if mle_complete is None:\n",
    "        mle_complete = mle_completed\n",
    "    else:\n",
    "        mle_complete = np.hstack([mle_complete, mle_completed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_complete[10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! Now we just have to reverse that <code>get_dummies</code> encoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_dummy(df_dummies):\n",
    "    pos = defaultdict(list)\n",
    "    vals = defaultdict(list)\n",
    "\n",
    "    for i, c in enumerate(df_dummies.columns):\n",
    "        if \"_\" in c:\n",
    "            k, v = c.split(\"_\", 1)\n",
    "            pos[k].append(i)\n",
    "            vals[k].append(v)\n",
    "        else:\n",
    "            pos[\"_\"].append(i)\n",
    "\n",
    "    df = pd.DataFrame({k: pd.Categorical.from_codes(\n",
    "                              np.argmax(df_dummies.iloc[:, pos[k]].values, axis=1),\n",
    "                              vals[k])\n",
    "                      for k in vals})\n",
    "\n",
    "    df[df_dummies.columns[pos[\"_\"]]] = df_dummies.iloc[:, pos[\"_\"]]\n",
    "    return df\n",
    "\n",
    "rev_df = reverse_dummy(pd.DataFrame(data=mle_complete, columns=dummy_df.columns))\n",
    "rev_df = rev_df[list(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much incorrect data points we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3807.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect = (rev_df != df)\n",
    "incorrect_cnts = incorrect.apply(pd.value_counts)\n",
    "incorrect_sum = incorrect_cnts.sum(axis=1)\n",
    "incorrect_sum[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, that sounds like a lot. Maybe we didn't do well overall? But how much are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17873"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = df_incomplete.apply(pd.isnull)\n",
    "missing_cnts = missing.apply(pd.value_counts)\n",
    "missing_sum = missing_cnts.sum(axis=1)\n",
    "missing_sum[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, that is a lot, too! Okay, I'll stop teasing you, here is the accuracy of our imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786997146534\n"
     ]
    }
   ],
   "source": [
    "accuracy = 1.0 - (incorrect_sum[1] / missing_sum[1])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 79%. Not bad considering the amount of data we have. But we haven't compared that to other approaches for data imputation (well, I have).\n",
    "\n",
    "# 4. Conclusion(s)\n",
    "\n",
    "And once again, the day is saved thanks to the powerp... the Autoencoder. It was quite fun and easy to build this model using Keras. Probably it will be even easier in the near future since Keras will be integrated directly into TensorFlow. Some questions that require further investigation remain unanswered: \n",
    "\n",
    "- Is this model effective for categorical data imputation? \n",
    "- Was our data highly correlated and easy to predict?\n",
    "- Can other models perform better? If so, when?\n",
    "- Can we incorporate uncertainty into our imputation? So we have a degree of belief of how good the imputed value is.\n",
    "\n",
    "You can think about those questions. I am gonna go eat some mushrooms.\n",
    "\n",
    "# References\n",
    "\n",
    "[Keras](https://keras.io/) - The library we used to build the Autoencoder<br/>\n",
    "[fancyimpute](https://github.com/hammerlab/fancyimpute/) - Most of the Autoencoder code is taken from this awesome library<br/>\n",
    "[Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/) - Unsupervised Feature Learning and Deep Learning on Autoencoders<br/>\n",
    "[Denoising Autoencoders](http://deeplearning.net/tutorial/dA.html) - Tutorial on Denoising Autoencoders with short review on Autoencoders<br/>\n",
    "[Data Imputation on Electronic Health Records](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5144587/) - Great use of Deep Autoencoders to impute medical records data<br/>\n",
    "[Data Imputation using Autoencoders in Biology](http://www.biorxiv.org/content/biorxiv/early/2016/06/07/054775.full.pdf) - Another great use of Autoencoders for imputation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
